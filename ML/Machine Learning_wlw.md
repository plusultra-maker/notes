# L1 introduction
## history
- Neural Networks
  - XOR problem
  - BP&DP
- VC Theory
- PAC learning

# L2 Concentration Inequalities
## single random variable
- Markov Inequality
  - 一阶矩
  - $$ P(|X| \geq \epsilon) \leq \frac{E[|X|]}{\epsilon}$$
- Chebyshev Inequality
  - 二阶矩  
  - $$P(|X-\mu| \geq \epsilon) \leq \frac{Var(X)}{\epsilon^2}$$
  - 总结一下，更多的information，更好的bound
- even better with more information
  - 更高阶矩:k次方和一次方的不等是等价的
  - $$P(|X-\mu| \geq \epsilon) \leq \frac{E[(X-\mu)^k]}{\epsilon^k}$$？
  - 如果我们知道所有阶矩的信息，我们能做什么
- Moment Generating Function
  - 引入一个变量t，来描述所以阶矩的信息，并可以找出最强的bound
  - 首先 如何描述这些阶矩的信息
  - $$E[e^{tX}]=\sum_{k=0}^{\infty} \frac{E[X^k]}{k!}t^k$$
  - （视作关于t的函数进行展开）
  - 从而我们把直接用t来指出t阶矩，换为用E$e^{tX}$以及t的任意性来利用这无穷多的信息，且其形式更简单，t连续可导
- so use $E[e^{tX}]$ to describe $P(x \geq k )$
  - $$P(X \geq k) \leq \inf_{t \geq 0} \frac{E[e^{tX}]}{e^{tk}}$$ (inf means the smallest value)
  - Chernoff inequality
  - 一个相当好的bound
## Concentration Inequalities (multiple random variable)
- Concentration Inequalities
  - 我们就想知道，1000次抛硬币，正面朝上的次数是多少是可以相信的，即平均值以多快的速度收敛到期望值，这是大数定理、中心极限定理都没有回答的问题
  - from chebyshev (e.g. Benoulli)
  - $$P(|\frac{1}{n}\sum_{i=1}^n X_i - \mu| \geq \epsilon) \leq \frac{Var(X)}{n\epsilon^2}$$
  - the question is , is $\frac{1}{n}$ the best we can do?
  - C.L.T lead us to Gaussian tail probability that might tell us that $e^{-O(X)}$ is a possible better form
  - C.L.T 指出了整个分布趋近于高斯分布，利用了整个分布的信息，而chebyshev只利用了期望和方差的信息，所以我们应该学着利用更多的信息来得到更好的bound，就像上面的Chernoff inequality
- somehow mentioned Entropy (Pre-knowledge)
  - use binary encode
  - Entropy
    - $$H(X)=-\sum_{x \in X} p(x) \log_{2}{p(x)}$$
    - $$H(X) \geq 0$$
    - $$H(X)=0 \iff p(x)=1$$
    - 代表了对于一个分布是p(x)的随机变量，我们需要多少bit的编码长度来表示它
  - relative entropy(KL divergence)
    - 有两个随机变量的分布p(x)和q(x)，描述其差异，仍是bit的长度
    - $$D(p||q)=\sum_{x \in X} p(x)log\frac{p(x)}{q(x)}$$
    - $$D(p||q) \geq 0$$
    - $$D(p||q)=0 \iff p=q$$
- can we use chernoff inequality on benoulli?
  - $$P(|\frac{1}{n}\sum_{i=1}^n X_i - \mu| \geq \epsilon) \leq e^{-nD_{KL}(\mu+\epsilon||\mu)}$$
  - Chernoff bound
- what if we only have X's expectation, x in [0,1], ex=p
  - still
- more, many x in different ex=p_i p_i's E=p,how?

- more on Chernoff bound
  - independent, $x_i \in [0,1]$, $E[x_i]=p_i$, 有 $\Sigma p_i=p$
    - use Jansen's inequality, the same as Chernoff bound
  - additive Chernoff bound
    - 改写为 $$P(\frac{1}{n}\sum_{i=1}^n X_i - p \geq \epsilon) \leq e^{-nD_{KL}(p+\epsilon||p)} \leq e^{-2n \epsilon^2}$$
    - 这样与p无关，是一个广泛的性质
  - 随机变量的Concentration究竟是如何发生的，其程度与什么相关——尾部收敛速度
  - Hoeffding Ineq:
    - 对于多个随机变量，独立，其只需满足对于每个随机变量$X_i$，其取值范围在$[a_i,b_i]$之间，即可得到其平均值的Concentration Inequality
    - $$P(|\frac{1}{n}\sum_{i=1}^n X_i - \mu| \geq \epsilon) \leq e^{\frac{-2n^2\epsilon^2 }{\sum_i(b_i-a_i)^2}}$$
    - 说明只需要有界而不需要其具体分布，就可以表现出一定的concentration
- Draw with/without replacement
  - Setting:
    - Assume $a_1,a_2,...,a_N \in [0,1]$
    - Randomly draw $X_1,X_2,...,X_n$ from $a_1,a_2,...,a_N$
  - with replacement  
    - 每次draw是完全一样的，X_i是独立且同分布的Bernoulli随机变量，其p就是$a_i$中1的比例，由前述有concentration
  - without replacement
    - 直观的想，without 会使结果更加concentrated，因为每次draw都会明确使得剩下的a_i的比例朝已知的方向变化，即降低了不确定性
  - 计算：
    - 记$X_i$为第i次draw with的结果，$Y_i$为第i次draw without的结果
    - 则证明$Y_i$更加concentrated，只需证明 
    - $$对于任意t有E[e^{t\Sigma Y_i}] \leq E[e^{t\Sigma X_i}] $$ 
    - 对$t$做taylor expansion:
      $$ E[e^{t\Sigma X_i}] = 1+t*E[\Sigma X_i]+t^2*E[\Sigma_{ij} X_iX_j]+...$$
      $$ E[e^{t\Sigma Y_i}] = 1+t*E[\Sigma Y_i]+t^2*E[\Sigma_{ij} Y_iY_j]+...$$
    - 对于0阶和1阶，显然相等（不涉及相关性）
    - 对于2阶以上，如二阶，$E[\Sigma_{ij} Y_iY_j]$即$Y_i=1,Y_j=1$的概率，而由于without replacement，出现一个1后，下一个1的概率就会降低，所以$E[\Sigma_{ij} Y_iY_j] \leq E[\Sigma_{ij} X_iX_j]$，更高阶是类似的
    - 因此，$E[e^{t\Sigma Y_i}] \leq E[e^{t\Sigma X_i}]$，即without replacement更加concentrated
    - 在之后Generation中有用
- more concentration operation
  - 之前的都是多个随机变量的平均值的concentration,还有什么对于多个随机变量的操作（函数）能产生concentration
  - stable function:以$X_1,X_2,...,X_n$为输入的函数，当只改变其中一个随机变量时，其输出的变化不会太大，称为stable，如求和求平均
  - McDiaraid Lamma
    - 如果对于一个stable function f，其有$|f(X_1,X_2,...,X_i,...,X_n)-f(X_1,X_2,...,Y_i,...,X_n)| \leq c_i$，则
    - $$P(|f(X_1,X_2,...,X_n)-E[f(X_1,X_2,...,X_n)]| \geq \epsilon) \leq e^{-\frac{\epsilon^2}{\Sigma c_i^2}}$$

# L3 VC Theory: The First Theory of Generalization
> From training data to test data
- Overview
  - 任何一个learned classifier能从i个training data中，而training data是采样自某个function的随机变量，学到使training error最小的classifier就能一定程度上使得test err最小。
  - $$err_{tr}=1/n \sum_{i=1}^n I[f(x_i) \neq y_i]$$
  - $$err_{test}=E_{x,y}[I[f(x) \neq y]]$$
  - 其原理是如果都是独立同分布，则training err应该会concentrate到test err附近
  - 但是当出现“过拟合”现象时，这个classifier在test data上的error可能很大，这是训练是根据数据产生的总error进行的，所以在过程中f(x_i)直接产生了相关性，过拟合就是因为过强的相关性使其偏离了独立随机变量，而失去了concentration
- Oversimplified setting
  - 假设|F|<∞ 即classifier的选取空间是有限的
  - 研究$$P(1/n \sum_{i=1}^n I[f(x_i) \neq y_i]-P(I[f(x) \neq y]) \geq \epsilon)$$
    $$\leq |F|e^{-2n\epsilon^2}$$
  - 其原理是每个固定的f，其training error是一个独立同分布的随机变量，所以可以用之前的concentration inequality，而|F|是有限的，所以可以用union bound,即任何一个f使得该事件发生的概率小于所有f使得该事件发生的概率之和----worst case analysis
  - 该推导的根本思路是f本质上是动态的，但作为F考虑时可以认为是固定的f的集合，因此可以用之前的concentration inequality；而对于指定的f其学习过程产生了相关性，所以concentration会变化
- Infinite Hypothesis Space (Step 1)
  - 想法是避免worst case是所有的f可能性，把问题转化到data的distinguishable classifier空间上，因为训练data总是有限的，因此这个空间也是有限的
  - 利用double sample trick 把随机变量和函数的概率差转为两组随机变量间的概率差
  - 这样的好处是事件内的所有变量都是数据的映射结果，而不是函数空间，因此这里case数只有data的distinguishable classifier空间的大小，记为$E(N^F(z_1,z_2,...,z_2n))$
  - 而随机变量间概率差的可以转换为一个draw without replacement的问题，即2n个数据中抽n个的concentration问题
- 接下来的问题就是$N^F(z_1,z_2,...,z_2n)<=N^F(2n)$记为最大值，$N^F(n)$代表了n个变量
- 接下来研究这个$N^F(n)$ (Step 2)
  - 在n比较小的时候F内的模型的表达能力足够，因此等于$2^n$，但是随着n的增大，表达能力有限，有一些pattern达不到，因此在某个$n>d$后这个值严格小于$2^n$
  - 对于n>d，有任何d+1个数据都不能被表示为所有0/1，因此$$N^F(n) \leq \sum_{i=0}^d C_{n}^i$$
  - 可进一步利用Bernulli的chernoff bound得到$$N^F(n) \leq (\frac{en}{d})^d$$
-  VCD: (Step 3)
  - d即最大的使得$N^F(n) =2^n$成立的n,记为VC Dimension
  - 结合前两步利用\delta换元整理就可以得到 $$P_D(Y \neq f(x)) \leq P_{S \sim D^n}(Y \neq f(x)) +O( \sqrt{\frac{d \ln(n)+\ln(1/\delta)}{n}})$$
- VCD：研究这个d对于F和dataset是如何决定的
  - on linear classfiers in $R^d$
    - 即$F=\{sign(w^Tx+b)|w \in R^d\}$
    - 可证得VCD=dim(w)+1
    - 一个线性分类器的VC Dimension就是其参数的个数，因为其参数的个数就是其表达能力的大小

# L4 Learning Algorithms
- linear classification
  - max margin classifier
    - 在正确分割的前提下，最大化最小间隔
    - 但其形式并不好 t不是我们能直接控制的变量
    - 因此我们想转化为一个关于w和b的凸优化问题
- Minimax theory 
  - 零和matrix博弈，分先后
  - pure strategy
    - A先选列，b观察后选行，A付给B,
      - $$A=min_{x}max_{y} (m_{xy})$$
    - 反之 b先 
      - $$B=max_{y}min_{x} (m_{xy})$$
    - $$A \geq B$$
  - 仍分先后，A付给B, mixed strategy
    - A有一个概率分布p，B根据A的分布有一个概率分布q，最后pay off是$p^T M q$
    - A先B后
      - $$A=min_{p}max_{q} (p^T M q)$$
    - B先A后
      - $$B=max_{q}min_{p} (p^T M q)$$
    - 可以证明两者相等，对应的p和q是相同的，即两方的最优选择是不分先后的
    - 更广泛的sion's minimax theorem
      - 只要f对于x是凸的，对于y是凹的，且f是有界的，则
      - $$min_{x}max_{y} (f(x,y))=max_{y}min_{x} (f(x,y))
- Lagrange duality
  - 原问题
    - $$min_{x}f(x)$$
    - $$s.t. h_i(x)=0$$
    - $$g_j(x) \leq 0$$
    - 其中f,g都是凸函数，h是线性函数
  - 使用Lagrange函数转化为Lagrange function
    - $$\min_x\max_{\alpha,\beta}L(x,\alpha,\beta)=f(x)+\sum_i \alpha_i h_i(x)+\sum_j \beta_j g_j(x)$$
    - $$s.t.\ \alpha_i \geq 0$$
    - 用Sion's minimax theorem可以证明Lagrange function可以转化为
    - $$\min_x\max_{\alpha,\beta}L(x,\alpha,\beta)=\max_{\alpha,\beta}\min_x L(x,\alpha,\beta)$$ 
    - 记第一步min_x的结果为$x^*=\varphi  (\alpha,\beta)$,展开后
    - $$\max_{\alpha,\beta}f(\varphi  (\alpha,\beta))+\sum_i \alpha_i h_i(\varphi  (\alpha,\beta))+\sum_j \beta_j g_j(\varphi  (\alpha,\beta))$$
    - $$ s.t.\ \alpha_i \geq 0$$
    - 即为对偶形式，转化为关于$\alpha,\beta$的优化问题
- 两对偶问题的解满足KKT条件
  - 四个式子，比较重要的是$\lambda_i g_i(x^*)=0$总是成立，即两者至少有一个为0，为互补松弛性
- SVM
  - 在max margin classifier转化为对偶问题的基础上，使用KKT条件，则w只在离超平面最近的点对应的$\lambda_i$的i上有值，这些点称为support vector
  >SVM的优势之一是它在高维空间中的表现良好，这意味着即使在数据不是线性可分的情况下，通过所谓的核函数（kernel function），SVM可以将数据映射到高维空间，从而使得数据在那个空间中线性可分。
- Bootstrap-Aggregation (Bagging)
  - 略
- AdaBoost
  - Boosting: Given a weak learner, boost it to a strong learner, also called ensemble.