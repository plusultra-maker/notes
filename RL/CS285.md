# notes aboyt me listening cs285
## L1
- RL: agent interacts with environment
  - 没有成型的监督数据，只有reward函数（可能也没有），归根结底一个ground-truth的世界，我们要找出策略实现最优的结果，这个结果通常用reward函数来评价
- RDL: deeplearning 强调其不需要人工进行分解，强调用网络直接实现端到端的学习；
  - 而RDL则更进一步，不止于用DL的结果来进行策略，而是从感知输入直接到策略的reward做端到端的学习，这相较直接用DL的不确定性结果来进行盲目的策略学习，对于策略的结果更高效
- 几个优化的方向
  - imitation: 从已有的好策略在模仿学习
  - prediction：我们不总是能去试错，需要学习去预测，这样可以减少试错的次数，减少不确定性
    - model-based: 用模型去预测
  - learn to learn
    - learn reward： 没有天然reward，人工设定效果未必好，需要学习好的reward设定

## L2 action supervision
- notation
  - policy: $\pi(a|s)$ or $\pi(a|o)$
    - state和observation的区别：state是agent的内部状态，observation是agent能观察到的环境的状态，observation是state的函数，但有时候你无法完全从observation中还原出state
  - Markov property: $P(s_{t+1}|s_t, a_t) 对于过去的state和action是独立的（无关的）
    - 但包含observation可能不满足Markov property，因为对observation的处理可能可以利用过去的observation信息以还原更准确的state信息
- Imitation Learnig
  - behavior cloning: 从已有的(o,a) pairs 中模仿策略
    - NOT WORK: 累计误差 (drifting)，只在expert见过的space内过拟合
  - modification: 用一些可以用来对小mistake进行feedback的训练方法
    - or 创造更general的data distribution (DAgger)
      - 区别于全部用expert的数据，DAgger用expert的数据来训练一个基本的policy，然后用训练出来的策略来采集更多的observation，让expert在这些observation来给出label（action），这样可以让策略在更general的数据分布上训练
      - 这可以比较好的收敛。但expert给出新的label这件事并不简单，在只有observation而不知道action的结果时expert并不一定能给出好的action
  - without more data --> high accuracy
    - why fail to fit expert's policy
      - export 可能不是MDP，可能利用过去的信息，因此MDP的policy就不能完全模仿
      - 