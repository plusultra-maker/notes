# 强化学习动手学 非常精简笔记
## 强化学习基础
### 多臂老虎机 
- 问题
  - K种action，对应K种奖励分布，但agent不知道分布
  - 没有状态
- 思路
  - 只需要通过尝试来估计每个action的奖励期望，从而找到最优action
  - 只需要估计期望，所以用平均值即可
- 新的问题--探索与利用的平衡
  - 探索需要代价，我只有有限的钱，怎么花尽量少的钱找到可能最好的action
  - 什么时候我认为找到了足够好的action，就从主要探索转为主要利用
  - 需要设计一个策略，来平衡探索和利用
- 策略
  - 最简单的策略：ε-greedy策略
    - 以ε的概率随机选择action，1-ε的概率选择当前估计最好的action
    - ε越大，探索越多，ε越小，利用越多
    - 所以ε是探索与利用的平衡参数，其如何选择和变化影响着累计奖励/懊悔
  - 后面两个我们不看了
  
### MDP Markov Decision Process
- setting
  - 有状态，有动作，有奖励，有状态转移机制
- 纯粹的马尔科夫环境 MRP
  - 马尔科夫性质：下一个状态只与当前状态有关，与之前的状态无关
  - 纯粹的马尔科夫过程：先不考虑动作，只考虑状态转移
  - 矩阵P：状态转移概率矩阵，P[s, s']表示从状态s转移到状态s'的概率
  - 加上奖励，这里奖励认为是到达s时的奖励，所以是r[s]
    - 折扣因子γ：未来奖励的重要性，0<=γ<=1，γ越大，未来奖励越重要
    - 根据此我们可以计算一个马尔可夫奖励过程的累计奖励，记为Gt
  - 价值函数
    - 所以在不考虑过程的情况下，我们可以计算一个状态的价值，记为V(s)，它表示从状态s开始的累计奖励的期望
    - 根据累积奖励的性质，$V(s) = r(s) + \gamma \sum_{s'}P[s, s']V(s')$
    - 也可以表示为矩阵形式：$V = R + \gamma PV$
    - 因此V是一个线性方程组的解，是有解析解的，计算复杂度是O(n^3)，所以太慢了
    - 当state很多时，我们可以用近似方法来计算V(s)，比如蒙特卡洛方法、TD方法、动态规划方法
- MDP
  - MDP是马尔科夫奖励过程的扩展，加上了动作, 来自agent
  - 状态转移概率矩阵变成了P[s, a, s']，表示在状态s下，采取动作a后，转移到状态s'的概率
  - Policy \pi
  - 价值函数
    - 状态价值函数：$ V_{\pi}(s) = E_{\pi}[G_t|S_t=s] $
      - 即当策略确定后，从状态s开始的累计奖励的期望
    - 动作价值函数：$ Q_{\pi}(s, a) = E_{\pi}[G_t|S_t=s, A_t=a] $
      - 这样细化了状态价值函数，表示在状态s下，采取动作a后的累计奖励的期望
    - 从而我们可以得到状态价值函数和动作价值函数之间的关系：$ V_{\pi}(s) = \sum_{a}\pi(a|s)Q_{\pi}(s, a) $
    - 并且得到类似的Bellman期望方程：
      - $ V_{\pi}(s) = \sum_{a}\pi(a|s)\sum_{s', r}P[s', r|s, a][r + \gamma V_{\pi}(s')] $
      - $ Q_{\pi}(s, a) = \sum_{s', r}P[s', r|s, a][r + \gamma \sum_{a'}\pi(a'|s')Q_{\pi}(s', a')] $
      - 其描述了V和Q各自的递归关系，基于这些我们后面设计算法去通过求解V和Q或类似的方法来最大化累计奖励
  - MDP和MRP
    - 当策略给定时，MDP退化为MRP，因为状态转移概率和奖励是确定的
    - 而问题在于策略是要进步的，如何在更新的策略中同时高效的更新V和Q是比较重要的
- 蒙特卡洛法
  - 适用于什么都不知道的情形，即不知道状态转移概率和奖励（黑盒）
  - 蒙特卡洛的基本思路就是采样，通过采样来估计期望，得到V和Q
  - 而即使是蒙特卡洛法也没有那么简单
    - 在采样过程中，我们会多次采样到重复的点，这些点如果已经采样过了就说明已经有了对其Q和V的估计，如何利用这些信息来减少采样次数同时保持估计的准确性是一个问题
- 占用度量