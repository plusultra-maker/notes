# 算分
## 数学基础
- 函数渐进界
  - 一堆符号和意义
  - 判断表达式的阶数和排序
    - >$log(n!)=nlog(n)$
    - 
- 递推方程求解
  - 主定理
    - 分类，注意f(n)中的logn系数往往导致无法被分类，必须手动递归树求解
    - 注意第三类（f(n)占大头），有附加条件$af(n/b)\leq cf(n)$


## 分治
1. 基本思路
   - 分开为数个子问题，分别求解并归并的递归实现
2. 分治的优化
   - 通过复用子问题的结论，减少需要求解的子问题个数or规模，从而减少分支数
   - 预处理，将归并需要的复杂度尽量移除递归过程，从而减少不断迭代的归并项，降低总复杂度
3. 分治的应用
   - 二分搜索
     - 要求：有界性（有界可分），单调性（可以验证是否符合条件，且一次性排除一半（一部分）不可能的情况）
## DP（参考cs170）
- 基本思路
   - 通常为优化问题
   - 要求：优化性质当问题可以被分解为子问题，且子问题可以
- To start with:Shortest paths in dags(directed acyclic graphs )
   - 在这种无环的图中，我们可以写为拓扑排序，线性的，只有向右的路径
     - which means 如果我们有了某个点v的所有左边点的$dist(u)$，我们可以很简单的求出$dist(v)$
     > $$dist(v) = min_{(u,v)∈E}{dist(u) + l(u, v)}$$
   - 所以只要我们有了这些子问题的解，就可以轻易的得到distv的解，这些问题嵌套着
   - 只要从小的从最开始的$dist(s)=0$开始解，则一步步就可以累积的把所有问题求完（有点像分治，但这里没有分这一步，结构是自然的，我们只需要逐步治而合
   - 在更普遍的问题里，dags is implicit，我们必须找到子问题与总问题的联系，并且将子问题递归到最简单的问题去：
      > Its nodes are the subproblems we define, and its edges are the dependencies between the subproblems:
- An example: Longest increasing subsequences
   - 最大的递增子序列，同样显示了有向性（无环），如果一个解子序列是最大的，那也就意味着以其倒数第二个点为终点的序列也是局部最大的，因此仍可以把大问题拆成一系列子问题的解的竞争，从而递归。
   - To myself:注意，子问题的解是局部最大的，不代表一定会选择这个子问题，例如：
      ```
         1，2，3，5，8，6，7
      ```
      这其中12358是局部最大的，12356也是，但因为最后是7，所有12358不构成7的子问题，因此不会被选中，这说明子问题的解是子问题的事情，只需要在子问题下是最值，而不需要考虑和其邻居的关系（不需要考虑为什么要选这个点为子问题）；而权衡如何从众多子问题中，选出并计算总问题的解，这是那个递归方程需要处理的问题。
   - Recursion? No, thanks.(unread)
     - 递推方程的建立不一定指向递归算法，因为递归会导致重复的计算相同的子问题，因为子问题是后出现的，所以很难复用
     - 
- Edit distance
   - Greedy algorithms build up a solution piece by piece, always choosing the next piece that offers the most obvious and immediate benefit.
   - And prove that's right by induction.
   - Mini

- Knapsack

- Hints
  - 先做离散的分割，使得最优解具有递推的性质
    - 有时不是一个显性的值的最优化问题，可能是可行性问题，但我们可以通过设计F={0,1}的递归函数来转化为最优化问题（有1则1）
  - 要有初始边界条件(i=1),和结束界限
  - 注意计算的过程是否可以优化
  - 

## Greedy
- 思路：
  - 以某种规则，正向的选择第一步的最优方案；
  - 完成第一步选择后，问题转换为相似的问题，再次重复上一步；直到完成所有选择。
  - 
- Hints
  1. 比较明显的问题，用数学归纳法证明
  2. 每当解答出于直觉，或对小范围成立时（补充题1），如果不方便直接进行递推和归纳，那考虑使用交换论证

## backtracking
- 思路
- Hints:
  - 回溯与DP: DP通常是基于两个变量的一个表，而回溯是基于向量的一个树

## linear programming
- 模型的一般形式
  - 目标函数和目标方向
  - 约束条件：线性不等式
  - 非负条件
  - 自由变量 无所谓非负
  - 可行解，可行域；最优解，最优值
- 标准形
  - 目标为min
  - 约束全部为等式，非线性项大于0
  - 所有变量要求非负
- 化为标准型
  - max -> min
  - 把b_i<0的式子两边变号，使得所有约束都有b_i>=0
  - 引入非负的松弛（剩余）变量，把不等式变为等式
  - 把自由变量$x_j->x_j^{'}-x_j^{''}$，后两者大于零
  - 这样就得到了可能有更多变量参与的标准形，接下来只需要研究标准形的性质即可
  - 实际上全是等式也给我们提供了用矩阵表示的便利性
- 标准形的可行解
  - 找到系数矩阵A$(r_A=m)$的m个线性无关向量，构成一组基B，则对应的变量为基变量
  - 令非基变量全为0，则基变量必然有唯一解$x_B=B^-1b$，称为基本解；
  - 检验如果基本解满足非负约束，则为基本可行解，对应的基为可行基
- 基本解的性质定理：
  - 有可行解，必有基本可行解（即必然可以通过满足约束的变化使得一些变量为0，这是线性方程组的性质）
  - 有最优解，必存在一个基本可行解是最优解（实际上即将“优化目标函数为最优”写为一个新的约束方程，则新的方程组有可行解，则必有满足最优条件的基本可行解）
- 单纯形法：
  - 基本思路：从一个可行解移动到一个具有更好目标函数值的相邻可行解（其实这是几乎所有最优化算法的基础）
    - 每个标准形的一组基选择对应着唯一的解，如果解可行，则对应着唯一的可行值，只要找到符合最优条件的可行值或证明没有满足最优条件的可行值，那就完全的解决了此问题（由定理2）
    - 因此此法把一个多维度的连续性问题转换为了一个组合数范围的离散问题，从而易于求解
    - 接下来的问题是：什么是最优条件；更简单的说，什么样的基变量变化可以降低目标函数值
  - 最优性检验：
    - 通过矩阵推导，可以在任意个给定的可行基B以及对应目标值z_0的基础上,得到:
      $$
      \begin{aligned}
      z=&c^Tx \\
      =&z_0+(c^T-c_{B}^T B^{-1} A)x \\
      从而记 \ 
      \lambda^T=&c^T-c_{B}^T B^{-1} A \ 为检验数 \\
      z=& z_0+\lambda^Tx \\
      \end{aligned}
      $$
    - 这样可以检验其他解（尤其是相邻解）相对此解是否更优（值更小）
    - 对于检验数，由于当x为对应B的变量取值时，所有非基变量都为0，而基变量都>0；
    - 同时将基变量项拆解可知，基变量对应的检验数必为0，因此x不变时，该差值自然为0
    - 这条性质告诉我们，原来的基变量如何取值不影响此差值，故只要我让一个检验数<0的非基变量变为非0，则目标值必然减小，这一推论告诉我们两件事：
      1. 检验数全部大于0，则原x是最优解
      2. 如果存在一个变量的检验数小于0，且$B^{-1} A$对应该值的列向量所有值都<=0，那么让此变量变得任意大于0，基变量组都仍有可行解，且差值可以任意减至负无穷，可以任意说明不存在最优解。
  - 替换策略：
    - 则如果存在检验值小于0且不满足上述2的非基变量，选取其中检验数绝对值最大的，选择任意基变量与之替换
    - 但为了保证替换后的基变量组仍有可行解，需要$B'^{-1}$的基变量系数大于0，则选取选定非基变量下，所有当前系数矩阵>0的项中 与非线性项比值最小的即可
    - 同时还可能写出检验数的变换，第j个变量的检验数$\lambda_j$的变化量为$-\lambda_k \alpha_{lj}/\alpha_{lk}$,即得到新的检验数；
      - 由于前面的条件，$\lambda_k<0$,以及$alpha_{lk}>0$,因此只需要考虑那些$\alpha_{lj}<0$或原$\lambda_j<0$的项，他们的检验数可能变化之后仍是负的
  - 单纯形表：注意化为标准型的min情况再来列表，$\lambda$的计算可以传递
- 对偶性
  - 有一个原始的max线性规划，则存在一个对偶的min线性规划，其形式是系数矩阵做转置，非线性项向量与目标函数系数向量互换，约束变为大于等于，并添加合适的变量数量
  - 如果给出的原规划存在大于等于或等于约束，则通过拆分和变号，全部转为原始形式，再做对偶，完成之后再化简
    - 可以推出一个对偶规划的一般形式，值得注意的是对偶后变量为任意的数量对应着对偶前的等式数量，对偶后的等式数量对应原规划中取值为任意的变量的数量
  - 对偶的性质：
    1. 原始规划的值总是小于对偶规划的值，证明即矩阵的结合律
    2. 当两值相等时，两不等式均取等，均达到最优解
    3. 一规划存在最优解，则两规划都有最优解，且最优值相等（即1的推论）
- 整数规划
  - 特点：要求一些特定的变量只能是整数取值
  - 分支限界：
    - 首先先去掉所有整数要求，求出最优解
    - 如果恰好满足整数要求，则结束
    - 否则选出某个不满足整数要求的变量最优解a，分支为加入小于a的下取整的规划，和大于a的上取整的规划，分别求解
    - 从而实现分支搜索，直到搜索到所有最优解满足整数要求的规划，比较这些叶子的最优值得到根的最优值
    - 优化：可以维护一个当前最优值的界，当子树的搜索已经必然劣于当前最优值了，则不必再搜下去了
  - 这样的方式可以将整数的取值搜索问题，简化为只发生在边界可行解上的线性规划解法，这样在近似意义上可以减少搜索空间
  

## Amortized Analysis
- 概念
  - 平摊分析不同于传统的最坏情况或平均情况分析，因为它考虑了一组操作的总体代价，而不仅仅是单个操作的代价。
- 聚集分析
  - 由于算法总是在一个客体上连续的运行，而一系列的操作前后之间是有联系和限制的，因而一组操作的总代价不是简单的单个操作的倍数，我们需要考虑操作间的联系
  - 在一些情形中，即使任何一步操作最差情况下都可以有很高的代价，但由于一些限制这些最坏操作发生的次数一定很小，那么它们的代价可以被平摊到其他操作上
  - 而这些时候，通常我们很难求出每一步出现多高代价的概率分布，但我们知道总体我们至多要完成哪些操作，所以总运行时间有一个紧的上界，从而求平均得到平摊时间
    - 这不是一种概率意义上的平均，而是一种分摊意义上的平均
    - 聚集分析不在乎具体的操作（通常跑完一个算法可能会用到好几种操作），只在乎总体的代价与总体的次数
- 记账法
  - 我们为不同的操作赋予一些假设代价，可能高于或低于实际代价。
  - 如果高于实际代价，就把差值当作存款存起来，可以存放到被操作的数据结构中的某些对象上
  - 这样当其他操作再次操作这些对象时，如果我们分配的代价不能满足它们的实际代价，那就利用对象上存的钱去填补这个空缺；而如果存放的机制设计的好，那可以得到某种不变性，保证总能找到存款去补偿
  - 如果所有操作都进行完毕我们总是能保证不欠钱，那就是一个正确的假设，而总假设的代价就是总真实代价的上界
    - 如果不能，则需要重新考虑假设代价与分配机制
- 势能法
  - 把记账法的思路抽象化一些，更朴实的存款策略是以一个数据结构的状态的函数去标注，称为势函数
  - 我们定义每一步的平摊代价是真实代价加势函数的增加值，类似物理中的输入的能量等于损耗加势能变化
  - $$a_i=c_i+\Phi(D_i)-\Phi(D_{i-1})$$
  - 这样 只要势函数满足末态势能高于初态，则说明平摊代价是合理的，其和是真实代价的和的上界
  - 总结：记账法和势能法都是类似的主动获得单个操作的平摊代价，而通常这些代价是常数阶的，这就说明总代价也只是总操作次数阶的，这就是平摊分析的价值，通过平摊而避免被罕见的大代价操作影响总代价估计，得到更合理的上界
- 实例：动态表
- Hints：
  - 势能函数法通常是最直接的方法，势能函数的构造往往也对应着数据结构与操作逻辑的某种性质，形式上与构造记账法的分摊存款是类似的，包括当操作逻辑本身带有数值的性质，可以很好的融合到势能函数中；但由于不用考虑过多的细节，只需计算差值，是比记账法更简洁的
  - 因此构造势能函数要考虑数据结构的结构特性与操作逻辑的特点（尤其是那些代价可能很大的操作）